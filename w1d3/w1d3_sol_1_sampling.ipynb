{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Day 3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "gpt = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sampling_methods(\n",
    "    input_ids: t.Tensor, logits: t.Tensor, temperature=1.0, freq_penalty=0.0, top_k=0, top_p=0.0\n",
    ") -> int:\n",
    "    '''\n",
    "    Return the next token, sampled from the model's probability distribution with modifiers.\n",
    "x\n",
    "    input_ids: shape (seq,)\n",
    "    '''\n",
    "    assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "    assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "    assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "    assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "    assert not (top_p != 0 and top_k != 0), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "    if temperature == 0:\n",
    "        return greedy_search(logits)\n",
    "    if temperature != 1.0:\n",
    "        logits = apply_temperature(logits, temperature)\n",
    "    if freq_penalty != 0.0:\n",
    "        logits = apply_freq_penalty(input_ids, logits, freq_penalty)\n",
    "    if top_k > 0:\n",
    "        return sample_top_k(logits, top_k)\n",
    "    if top_p > 0:\n",
    "        return sample_top_p(logits, top_p)\n",
    "    return sample_basic(logits)\n",
    "\n",
    "def sample_tokens(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    initial_text: str,\n",
    "    max_tokens_generated: int = 30,\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    '''\n",
    "    Sample tokens until the model outputs `tokenizer.eos_token_id` or the specified token limit is reached.\n",
    "\n",
    "    Return: the prompt and continuation concatenated\n",
    "    '''\n",
    "    model.eval()\n",
    "    input_ids: list = tokenizer.encode(initial_text)\n",
    "    generated = []\n",
    "    device = next(model.parameters()).device\n",
    "    for _ in range(max_tokens_generated):\n",
    "        new_input_ids = t.tensor(input_ids + generated, dtype=t.int64, device=device)\n",
    "        new_input_ids_truncated = new_input_ids[-min(tokenizer.model_max_length, new_input_ids.shape[0]):].unsqueeze(0)\n",
    "        output = model(new_input_ids_truncated)\n",
    "        all_logits = output if isinstance(output, t.Tensor) else output.logits\n",
    "        logits = all_logits[0, -1]\n",
    "        new_token = apply_sampling_methods(new_input_ids, logits, **kwargs)\n",
    "        assert isinstance(new_token, int)\n",
    "        generated.append(new_token)\n",
    "        if new_token == getattr(tokenizer, \"eos_token_id\", None):\n",
    "            break\n",
    "    return tokenizer.decode(input_ids + generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt:  Jingle bells, jingle bells, jingle all the way\n",
      "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Greedy decoding a second time (should be deterministic): \n",
      "Your model said: Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def greedy_search(logits: t.Tensor) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: the most likely token (as an integer)\n",
    "    '''\n",
    "    return int(logits.argmax(dim=-1).squeeze())\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(\"Greedy decoding with prompt: \", prompt)\n",
    "output = sample_tokens(gpt, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output}\")\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Greedy decoding a second time (should be deterministic): \")\n",
    "output = sample_tokens(gpt, tokenizer, prompt, max_tokens_generated=8, temperature=0.0)\n",
    "print(f\"Your model said: {output}\")\n",
    "expected = \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0975, 0.1987, 0.3049, 0.3988])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_basic(logits: t.Tensor) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "\n",
    "    Return: a sampled token\n",
    "    '''\n",
    "    distribution = t.distributions.categorical.Categorical(logits=logits)\n",
    "    return int(distribution.sample())\n",
    "\n",
    "N = 20000\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_basic(unnormalized_logits) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
    "t.testing.assert_close(counts, probs, atol=0.01, rtol=0)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def apply_temperature(logits: t.Tensor, temperature: float) -> t.Tensor:\n",
    "    '''\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: shape (vocab_size, )\n",
    "    '''\n",
    "    assert temperature > 0\n",
    "    logits = logits / temperature\n",
    "    return logits\n",
    "\n",
    "logits = t.tensor([1, 2]).log()\n",
    "cold_logits = apply_temperature(logits, 0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "hot_logits = apply_temperature(logits, 1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def apply_freq_penalty(input_ids: t.Tensor, logits: t.Tensor, freq_penalty: float) -> t.Tensor:\n",
    "    '''\n",
    "    input_ids: shape (seq, )\n",
    "    logits: shape (vocab_size, )\n",
    "\n",
    "    Return: shape (vocab_size, )\n",
    "    '''\n",
    "    vocab_size = logits.shape[0]\n",
    "    counts = t.bincount(input=input_ids, minlength=vocab_size)\n",
    "    \n",
    "    return logits - counts * freq_penalty\n",
    "\n",
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\").squeeze()\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = apply_freq_penalty(input_ids, logits, 2.0)\n",
    "assert penalized_logits[5156].item() == -11, \"Expected 6 occurrences of ' baby' with leading space\"\n",
    "assert penalized_logits[14801].item() == -5, \"Expected 3 occurrences of ' Baby' with leading space\"\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 with: High freq penalty ({'freq_penalty': 100.0}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool connecting to sane life. If we not only observe the hyper-anxiety of ozone depletion rights for decades before laying waste'\n",
      "\n",
      "Sample 0 with: Negative freq penalty ({'freq_penalty': -1.0}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool.\\n\\n\\nWith that in mind,, I said, let us be aware, and, as Belial,,'\n",
      "\n",
      "Sample 0 with: Too hot! ({'temperature': 2.0}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool cures bigotry, totally submerged outrage oxy fren opp395 functioning kidney puts false concessions restrict national control,, sponssudoerson pressure y'\n",
      "\n",
      "Sample 0 with: Pleasantly cool ({'temperature': 0.7}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool of a toy.\\n\\nThe world does not have a beginning yet, but a future, and it will be a'\n",
      "\n",
      "Sample 0 with: Pleasantly warm ({'temperature': 0.9}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool that never ends. The platform beneath us will never be bright, and, if we can overcome the corruption that keeps it'\n",
      "\n",
      "Sample 0 with: Too cold! ({'temperature': 0.01}):\n",
      "Your model said: 'A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool, and shall be able to walk on the earth.\\n\\nThe earth will be a place of great beauty, and'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 1\n",
    "your_prompt = \"A day will come when beings, now latent in our thoughts and hidden in our loins, shall stand upon Earth as a footstool\"\n",
    "cases = [\n",
    "    (\"High freq penalty\", dict(freq_penalty=100.0)),\n",
    "    (\"Negative freq penalty\", dict(freq_penalty=-1.0)),\n",
    "    (\"Too hot!\", dict(temperature=2.0)),\n",
    "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
    "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
    "    (\"Too cold!\", dict(temperature=0.01)),\n",
    "]\n",
    "for (name, kwargs) in cases:\n",
    "    for i in range(N_RUNS):\n",
    "        output = sample_tokens(gpt, tokenizer, your_prompt, max_tokens_generated=24, **kwargs)\n",
    "        print(f\"Sample {i} with: {name} ({kwargs}):\")\n",
    "        print(f\"Your model said: {repr(output)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2192, 0.3328, 0.4480])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_top_k(logits: t.Tensor, top_k: int) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "    top_k: only consider this many of the most likely tokens for sampling\n",
    "\n",
    "    Return: a sampled token\n",
    "    '''\n",
    "    top_logits = t.topk(logits, k=top_k)\n",
    "    sample_idx = t.distributions.categorical.Categorical(logits=top_logits.values).sample()\n",
    "    return int(top_logits.indices[sample_idx])\n",
    "\n",
    "k = 3\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_top_k(unnormalized_logits, k) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "expected = probs.clone()\n",
    "expected[:-k] = 0\n",
    "expected /= expected.sum()\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
    "t.testing.assert_close(counts, expected, atol=0.01, rtol=0)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model said: 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\n\"I just wonder if they were really communicating with each other?\" said Dr. Alvaro.\\n\\nThe scientists also learned that the unicorns also have a penchant for sex and often share their reproductive organs with a male or female partner.\\n\\nThe scientists believe that this is because the unicorns are'\n"
     ]
    }
   ],
   "source": [
    "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "output = sample_tokens(gpt, tokenizer, your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
    "print(f\"Your model said: {repr(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p of 0.5 or lower should only return token 2:  tensor([0., 0., 1.])\n",
      "top_p in (0.5, 0.8] should return tokens 1 and 2:  tensor([0.0000, 0.3820, 0.6180])\n",
      "Checking empirical frequencies (try to increase N if this test fails):  tensor([0.0000, 0.0000, 0.2272, 0.3345, 0.4383])\n",
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def sample_top_p(logits: t.Tensor, top_p: float, min_tokens_to_keep: int = 1) -> int:\n",
    "    '''\n",
    "    logits: shape (vocab_size, ) - unnormalized log-probabilities\n",
    "\n",
    "    Return: a sampled token\n",
    "    '''\n",
    "    sorted_logits, logit_indices = logits.sort(descending=True)\n",
    "    cumulative = sorted_logits.softmax(-1).cumsum(dim=-1)\n",
    "\n",
    "    select_count = max(t.searchsorted(cumulative, top_p, right=False).item()+1, min_tokens_to_keep)\n",
    "    select_indices = logit_indices[:select_count]\n",
    "    select_logits = logits[select_indices]\n",
    "\n",
    "    sample_idx = t.distributions.categorical.Categorical(logits=select_logits).sample()\n",
    "\n",
    "    return int(select_indices[sample_idx])\n",
    "\n",
    "\n",
    "N = 2000\n",
    "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, 0.5) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
    "print(\"top_p of 0.5 or lower should only return token 2: \", counts)\n",
    "assert counts[0] == 0 and counts[1] == 0\n",
    "\n",
    "N = 2000\n",
    "unnormalized_logits = t.tensor([0.2, 0.3, 0.5]).log() + 2.3456\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, 0.50001) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(unnormalized_logits)) / N\n",
    "print(\"top_p in (0.5, 0.8] should return tokens 1 and 2: \", counts)\n",
    "assert counts[0] == 0\n",
    "\n",
    "N = 4000\n",
    "top_p = 0.71\n",
    "probs = t.linspace(0, 0.4, 5)\n",
    "unnormalized_logits = probs.log() + 1.2345\n",
    "samples = t.tensor([sample_top_p(unnormalized_logits, top_p) for _ in range(N)])\n",
    "counts = t.bincount(samples, minlength=len(probs)) / N\n",
    "expected = probs.clone()\n",
    "expected[0:2] = 0\n",
    "expected /= expected.sum()\n",
    "print(\"Checking empirical frequencies (try to increase N if this test fails): \", counts)\n",
    "t.testing.assert_close(counts, expected, atol=0.01, rtol=0.0)\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model said: 'Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for his work on the cognitive-behavioral neuroscience of creativity (in particular, his work on the self-referential attribution of positive and negative experiences) and, most recently, the paper that led to the creation of AI in the first place. He also holds a Ph.D. in Cognitive Science from the University of'\n"
     ]
    }
   ],
   "source": [
    "your_prompt = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
    "output = sample_tokens(gpt, tokenizer, your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64)\n",
    "print(f\"Your model said: {repr(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
