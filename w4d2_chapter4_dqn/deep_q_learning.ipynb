{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/ipywidgets/widgets/widget_selection.py:9: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, Iterable\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from distutils.util import strtobool\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from gym.spaces import Discrete, Box\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from numpy.random import Generator\n",
    "import gym.envs.registration\n",
    "import pandas as pd\n",
    "import utils\n",
    "from utils_tabular import make_env\n",
    "import wandb\n",
    "\n",
    "t.set_default_dtype(t.float32)\n",
    "\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=4, out_features=180, bias=True)\n",
      "  (fc2): Linear(in_features=180, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Total number of parameters: 22862\n",
      "You should manually verify network is Linear-ReLU-Linear-ReLU-Linear\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal number of parameters: \u001b[39m\u001b[39m{\u001b[39;00mn_params\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYou should manually verify network is Linear-ReLU-Linear-ReLU-Linear\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[39massert\u001b[39;00m n_params \u001b[39m==\u001b[39m \u001b[39m10934\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_observation: int,\n",
    "        num_actions: int,\n",
    "        hidden_sizes: list[int] = [180, 120],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim_observation, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], num_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.relu(self.fc2(out))\n",
    "\n",
    "        return self.fc3(out)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    net = QNetwork(dim_observation=4, num_actions=2)\n",
    "    n_params = sum((p.nelement() for p in net.parameters()))\n",
    "    print(net)\n",
    "    print(f\"Total number of parameters: {n_params}\")\n",
    "    print(\"You should manually verify network is Linear-ReLU-Linear-ReLU-Linear\")\n",
    "    assert n_params == 10934\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayBufferSamples:\n",
    "    \"\"\"\n",
    "    Samples from the replay buffer, converted to PyTorch for use in neural network \n",
    "    training.\n",
    "    obs: shape (sample_size, *observation_shape), dtype t.float\n",
    "    actions: shape (sample_size, ) dtype t.int\n",
    "    rewards: shape (sample_size, ), dtype t.float\n",
    "    dones: shape (sample_size, ), dtype t.bool\n",
    "    next_observations: shape (sample_size, *observation_shape), dtype t.float\n",
    "    \"\"\"\n",
    "\n",
    "    observations: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    rewards: t.Tensor\n",
    "    dones: t.Tensor\n",
    "    next_observations: t.Tensor\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    rng: Generator\n",
    "    observations: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    rewards: t.Tensor\n",
    "    dones: t.Tensor\n",
    "    next_observations: t.Tensor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int,\n",
    "        num_actions: int,\n",
    "        observation_shape: tuple,\n",
    "        num_environments: int,\n",
    "        seed: int,\n",
    "    ):\n",
    "        assert (\n",
    "            num_environments == 1\n",
    "        ), \"This buffer only supports SyncVectorEnv with 1 environment inside.\"\n",
    "        self.observations = t.zeros((buffer_size, *observation_shape), dtype=t.float32)\n",
    "        self.actions = t.zeros((buffer_size,), dtype=t.int64)\n",
    "        self.rewards = t.zeros((buffer_size,), dtype=t.float32)\n",
    "        self.dones = t.zeros((buffer_size,), dtype=t.bool)\n",
    "        self.next_observations = t.zeros(\n",
    "            (buffer_size, *observation_shape), dtype=t.float32\n",
    "        )\n",
    "        self.buffer_pointer = 0\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def right_push_to_tensor(self, tensor, x):\n",
    "        return torch.cat((tensor[1:], t.tensor([x], dtype=tensor.dtype)))\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        actions: np.ndarray,\n",
    "        rewards: np.ndarray,\n",
    "        dones: np.ndarray,\n",
    "        next_obs: np.ndarray,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        obs: shape (num_environments, *observation_shape)\n",
    "            Observation before the action\n",
    "        actions: shape (num_environments, )\n",
    "            Action chosen by the agent\n",
    "        rewards: shape (num_environments, )\n",
    "            Reward after the action\n",
    "        dones: shape (num_environments, )\n",
    "            If True, the episode ended and was reset automatically\n",
    "        next_obs: shape (num_environments, *observation_shape)\n",
    "            Observation after the action\n",
    "            If done is True, this should be the terminal observation, NOT the first \n",
    "            observation of the next episode.\n",
    "        \"\"\"\n",
    "        pointer = self.buffer_pointer % self.buffer_size\n",
    "        self.observations[pointer] = t.tensor(obs)\n",
    "        self.actions[pointer] = t.tensor(actions)\n",
    "        self.rewards[pointer] = t.tensor(rewards)\n",
    "        self.dones[pointer] = t.tensor(dones)\n",
    "        self.next_observations[pointer] = t.tensor(next_obs)\n",
    "\n",
    "        self.buffer_pointer += 1\n",
    "\n",
    "    def sample(self, sample_size: int, device: t.device) -> ReplayBufferSamples:\n",
    "        \"\"\"Uniformly sample sample_size entries from the buffer and convert them to \n",
    "           PyTorch tensors on device.\n",
    "\n",
    "        Sampling is with replacement, and sample_size may be larger than the buffer size.\n",
    "        \"\"\"\n",
    "        sample_idx = self.rng.integers(\n",
    "            low=0, high=min(self.buffer_pointer, self.buffer_size), size=sample_size\n",
    "        )\n",
    "        obs = self.observations[sample_idx].to(device)\n",
    "        act = self.actions[sample_idx].to(device)\n",
    "        rew = self.rewards[sample_idx].to(device)\n",
    "        don = self.dones[sample_idx].to(device)\n",
    "        nex_obs = self.next_observations[sample_idx].to(device)\n",
    "\n",
    "        return ReplayBufferSamples(obs, act, rew, don, nex_obs)\n",
    "\n",
    "\n",
    "# if MAIN:\n",
    "#     utils.test_replay_buffer_single(ReplayBuffer)\n",
    "#     utils.test_replay_buffer_deterministic(ReplayBuffer)\n",
    "#     utils.test_replay_buffer_wraparound(ReplayBuffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if MAIN:\n",
    "#     rb = ReplayBuffer(\n",
    "#         buffer_size=256,\n",
    "#         num_actions=2,\n",
    "#         observation_shape=(4,),\n",
    "#         num_environments=1,\n",
    "#         seed=0,\n",
    "#     )\n",
    "#     envs = gym.vector.SyncVectorEnv(\n",
    "#         [utils.make_env(\"CartPole-v1\", 0, 0, False, \"test\")]\n",
    "#     )\n",
    "#     obs = envs.reset()\n",
    "#     for i in range(512):\n",
    "#         actions = np.array([0])\n",
    "#         (next_obs, rewards, dones, infos) = envs.step(actions)\n",
    "#         real_next_obs = next_obs.copy()\n",
    "#         for (i, done) in enumerate(dones):\n",
    "#             if done:\n",
    "#                 real_next_obs[i] = infos[i][\"terminal_observation\"]\n",
    "#         rb.add(obs, actions, rewards, dones, next_obs)\n",
    "#         obs = next_obs\n",
    "#     sample = rb.sample(128, t.device(\"cpu\"))\n",
    "#     columns = [\"cart_pos\", \"cart_v\", \"pole_angle\", \"pole_v\"]\n",
    "#     df = pd.DataFrame(rb.observations, columns=columns)\n",
    "#     df.plot(subplots=True, title=\"Replay Buffer\")\n",
    "#     df2 = pd.DataFrame(sample.observations, columns=columns)\n",
    "#     df2.plot(subplots=True, title=\"Shuffled Replay Buffer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(\n",
    "    current_step: int,\n",
    "    start_e: float,\n",
    "    end_e: float,\n",
    "    exploration_fraction: float,\n",
    "    total_timesteps: int,\n",
    ") -> float:\n",
    "    \"\"\"Return the appropriate epsilon for the current step.\n",
    "\n",
    "    Epsilon should be start_e at step 0 and decrease linearly to end_e at step \n",
    "    (exploration_fraction * total_timesteps).\n",
    "\n",
    "    It should stay at end_e for the rest of the episode.\n",
    "    \"\"\"\n",
    "    return start_e + (end_e - start_e) * min(\n",
    "        current_step / (exploration_fraction * total_timesteps), 1\n",
    "    )\n",
    "\n",
    "\n",
    "# if MAIN:\n",
    "#     epsilons = [\n",
    "#         linear_schedule(\n",
    "#             step, start_e=1.0, end_e=0.05, exploration_fraction=0.5, total_timesteps=500\n",
    "#         )\n",
    "#         for step in range(500)\n",
    "#     ]\n",
    "#     utils.test_linear_schedule(linear_schedule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(\n",
    "    envs: gym.vector.SyncVectorEnv,\n",
    "    q_network: QNetwork,\n",
    "    rng: Generator,\n",
    "    obs: t.Tensor,\n",
    "    epsilon: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"With probability epsilon, take a random action. Otherwise, take a greedy action \n",
    "        according to the q_network.\n",
    "    Inputs:\n",
    "        envs : gym.vector.SyncVectorEnv, the family of environments to run against\n",
    "        q_network : QNetwork, the network used to approximate the Q-value function\n",
    "        obs : The current observation\n",
    "        epsilon : exploration percentage\n",
    "    Outputs:\n",
    "        actions: (n_environments, ) the sampled action for each environment.\n",
    "    \"\"\"\n",
    "    determiner = rng.random()\n",
    "    n_envs = envs.num_envs\n",
    "    n_actions = envs.single_action_space.n\n",
    "    if determiner < epsilon:\n",
    "        return rng.integers(0, n_actions, n_envs)\n",
    "    else:\n",
    "        out = q_network(obs).detach().numpy()\n",
    "        out = out.argmax(axis=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "# if MAIN:\n",
    "#     utils.test_epsilon_greedy_policy(epsilon_greedy_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment Probe1-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "ObsType = np.ndarray\n",
    "ActType = int\n",
    "\n",
    "\n",
    "class Probe1(gym.Env):\n",
    "    \"\"\"One action, observation of [0.0], one timestep long, +1 reward.\n",
    "\n",
    "    We expect the agent to rapidly learn that the value of the constant [0.0] \n",
    "    observation is +1.0. Note we're using a continuous observation space for consistency \n",
    "    with CartPole.\n",
    "    \"\"\"\n",
    "\n",
    "    action_space: Discrete\n",
    "    observation_space: Box\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(np.array([0]), np.array([0]))\n",
    "        self.action_space = Discrete(1)\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        return (np.array([0]), 1.0, True, {})\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        super().reset(seed=seed)\n",
    "        if return_info:\n",
    "            return (np.array([0.0]), {})\n",
    "        return np.array([0.0])\n",
    "\n",
    "\n",
    "gym.envs.registration.register(id=\"Probe1-v0\", entry_point=Probe1)\n",
    "if MAIN:\n",
    "    env = gym.make(\"Probe1-v0\")\n",
    "    assert env.observation_space.shape == (1,)\n",
    "    assert env.action_space.shape == ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment Probe2-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment Probe3-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment Probe4-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment Probe5-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    }
   ],
   "source": [
    "class Probe2(gym.Env):\n",
    "    \"\"\"One action, observation of [-1.0] or [+1.0], one timestep long, reward equals \n",
    "       observation.\n",
    "\n",
    "    We expect the agent to rapidly learn the value of each observation is equal to the \n",
    "    observation.\n",
    "    \"\"\"\n",
    "\n",
    "    action_space: Discrete\n",
    "    observation_space: Box\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        pass\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "gym.envs.registration.register(id=\"Probe2-v0\", entry_point=Probe2)\n",
    "\n",
    "\n",
    "class Probe3(gym.Env):\n",
    "    \"\"\"One action, [0.0] then [1.0] observation, two timesteps, +1 reward at the end.\n",
    "\n",
    "    We expect the agent to rapidly learn the discounted value of the initial observation.\n",
    "    \"\"\"\n",
    "\n",
    "    action_space: Discrete\n",
    "    observation_space: Box\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        pass\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "gym.envs.registration.register(id=\"Probe3-v0\", entry_point=Probe3)\n",
    "\n",
    "\n",
    "class Probe4(gym.Env):\n",
    "    \"\"\"Two actions, [0.0] observation, one timestep, reward is -1.0 or +1.0 dependent on \n",
    "       the action.\n",
    "\n",
    "    We expect the agent to learn to choose the +1.0 action.\n",
    "    \"\"\"\n",
    "\n",
    "    action_space: Discrete\n",
    "    observation_space: Box\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        pass\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "gym.envs.registration.register(id=\"Probe4-v0\", entry_point=Probe4)\n",
    "\n",
    "\n",
    "class Probe5(gym.Env):\n",
    "    \"\"\"Two actions, random 0/1 observation, one timestep, reward is 1 if action equals \n",
    "       observation otherwise -1.\n",
    "\n",
    "    We expect the agent to learn to match its action to the observation.\n",
    "    \"\"\"\n",
    "\n",
    "    action_space: Discrete\n",
    "    observation_space: Box\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self, action: ActType) -> tuple[ObsType, float, bool, dict]:\n",
    "        pass\n",
    "\n",
    "    def reset(\n",
    "        self, seed: Optional[int] = None, return_info=False, options=None\n",
    "    ) -> Union[ObsType, tuple[ObsType, dict]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "gym.envs.registration.register(id=\"Probe5-v0\", entry_point=Probe5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try running this file from the command line instead: python <filename of this script> --help\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DQNArgs:\n",
    "    exp_name: str = os.path.basename(\n",
    "        globals().get(\"__file__\", \"DQN_implementation\").rstrip(\".py\")\n",
    "    )\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"Curt-CartPoleDQN\"\n",
    "    wandb_entity: Optional[str] = None\n",
    "    capture_video: bool = True\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    buffer_size: int = 10000\n",
    "    gamma: float = 0.99\n",
    "    target_network_frequency: int = 500\n",
    "    batch_size: int = 128\n",
    "    start_e: float = 1.0\n",
    "    end_e: float = 0.05\n",
    "    exploration_fraction: float = 0.5\n",
    "    learning_starts: int = 10000\n",
    "    train_frequency: int = 10\n",
    "\n",
    "\n",
    "arg_help_strings = dict(\n",
    "    exp_name=\"the name of this experiment\",\n",
    "    seed=\"seed of the experiment\",\n",
    "    torch_deterministic=\"if toggled, `torch.backends.cudnn.deterministic=False`\",\n",
    "    cuda=\"if toggled, cuda will be enabled by default\",\n",
    "    track=\"if toggled, this experiment will be tracked with Weights and Biases\",\n",
    "    wandb_project_name=\"the wandb's project name\",\n",
    "    wandb_entity=\"the entity (team) of wandb's project\",\n",
    "    capture_video=\"whether to capture videos of the agent performances (check out `videos` folder)\",\n",
    "    env_id=\"the id of the environment\",\n",
    "    total_timesteps=\"total timesteps of the experiments\",\n",
    "    learning_rate=\"the learning rate of the optimizer\",\n",
    "    buffer_size=\"the replay memory buffer size\",\n",
    "    gamma=\"the discount factor gamma\",\n",
    "    target_network_frequency=\"the timesteps it takes to update the target network\",\n",
    "    batch_size=\"the batch size of samples from the replay memory\",\n",
    "    start_e=\"the starting epsilon for exploration\",\n",
    "    end_e=\"the ending epsilon for exploration\",\n",
    "    exploration_fraction=\"the fraction of `total-timesteps` it takes from start-e to go end-e\",\n",
    "    learning_starts=\"timestep to start learning\",\n",
    "    train_frequency=\"the frequency of training\",\n",
    ")\n",
    "toggles = [\"torch_deterministic\", \"cuda\", \"track\", \"capture_video\"]\n",
    "\n",
    "\n",
    "def parse_args(arg_help_strings=arg_help_strings, toggles=toggles) -> DQNArgs:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    for (name, field) in DQNArgs.__dataclass_fields__.items():\n",
    "        flag = \"--\" + name.replace(\"_\", \"-\")\n",
    "        type_function = (\n",
    "            field.type if field.type != bool else lambda x: bool(strtobool(x))\n",
    "        )\n",
    "        toggle_kwargs = {\"nargs\": \"?\", \"const\": True} if name in toggles else {}\n",
    "        parser.add_argument(\n",
    "            flag,\n",
    "            type=type_function,\n",
    "            default=field.default,\n",
    "            help=arg_help_strings[name],\n",
    "            **toggle_kwargs,\n",
    "        )\n",
    "    return DQNArgs(**vars(parser.parse_args()))\n",
    "\n",
    "\n",
    "def setup(\n",
    "    args: DQNArgs,\n",
    ") -> Tuple[str, SummaryWriter, np.random.Generator, t.device, gym.vector.SyncVectorEnv]:\n",
    "    \"\"\"Helper function to set up useful variables for the DQN implementation\"\"\"\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\"\n",
    "        % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    )\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    rng = np.random.default_rng(args.seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [utils.make_env(args.env_id, args.seed, 0, args.capture_video, run_name)]\n",
    "    )\n",
    "    assert isinstance(\n",
    "        envs.single_action_space, Discrete\n",
    "    ), \"only discrete action space is supported\"\n",
    "    return (run_name, writer, rng, device, envs)\n",
    "\n",
    "\n",
    "def log(\n",
    "    writer: SummaryWriter,\n",
    "    start_time: float,\n",
    "    step: int,\n",
    "    predicted_q_vals: t.Tensor,\n",
    "    loss: Union[float, t.Tensor],\n",
    "    infos: Iterable[dict],\n",
    "    epsilon: float,\n",
    "):\n",
    "    \"\"\"Helper function to write relevant info to TensorBoard logs, and print some things to stdout\"\"\"\n",
    "    if step % 100 == 0:\n",
    "        writer.add_scalar(\"losses/td_loss\", loss, step)\n",
    "        writer.add_scalar(\"losses/q_values\", predicted_q_vals.mean().item(), step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(step / (time.time() - start_time)), step)\n",
    "        if step % 10000 == 0:\n",
    "            print(\"SPS:\", int(step / (time.time() - start_time)))\n",
    "    episodic_return = 0\n",
    "    for info in infos:\n",
    "        if \"episode\" in info.keys():\n",
    "            print(f\"global_step={step}, episodic_return={info['episode']['r']}\")\n",
    "            episodic_return = info['episode']['r']\n",
    "            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], step)\n",
    "            writer.add_scalar(\"charts/epsilon\", epsilon, step)\n",
    "            break\n",
    "    # OPTIONAL: ADD CODE HERE TO LOG TO WANDB\n",
    "    wandb.log({\"loss\":loss, \"episodic_return\":episodic_return})\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(\n",
    "            f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\"\n",
    "        )\n",
    "        args = DQNArgs()\n",
    "    else:\n",
    "        args = parse_args()\n",
    "    # train_dqn(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(args: DQNArgs):\n",
    "    (run_name, writer, rng, device, envs) = setup(args)\n",
    "    \"YOUR CODE: Create your Q-network, Adam optimizer, and replay buffer here.\"\n",
    "    n_actions = envs.single_action_space.n\n",
    "    obs_dim = envs.single_observation_space.shape\n",
    "    num_obs = np.array(obs_dim, dtype=int).prod()\n",
    "\n",
    "    q_network = QNetwork(num_obs, n_actions).to(device)\n",
    "    target_network = QNetwork(num_obs, n_actions).to(device)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    rb = ReplayBuffer(args.buffer_size, n_actions, obs_dim, envs.num_envs, args.seed)\n",
    "    optimizer = t.optim.Adam(q_network.parameters(), args.learning_rate)\n",
    "\n",
    "    start_time = time.time()\n",
    "    obs = envs.reset()\n",
    "    for step in range(args.total_timesteps):\n",
    "        \"YOUR CODE: Sample actions according to the epsilon greedy policy using the linear schedule for epsilon, and then step the environment\"\n",
    "        epsilon = linear_schedule(step, args.start_e, args.end_e, args.exploration_fraction, args.total_timesteps)\n",
    "        actions = epsilon_greedy_policy(envs, q_network, rng, t.Tensor(obs).to(dtype=t.float32).to(device), epsilon)\n",
    "        next_obs, rewards, dones, infos = envs.step(actions)\n",
    "\n",
    "        \"Boilerplate to handle the terminal observation case\"\n",
    "        real_next_obs = next_obs.copy()\n",
    "        for (i, done) in enumerate(dones):\n",
    "            if done:\n",
    "                real_next_obs[i] = infos[i][\"terminal_observation\"]\n",
    "                \n",
    "        rb.add(obs, actions, rewards, dones, next_obs)\n",
    "        obs = next_obs\n",
    "        if step > args.learning_starts and step % args.train_frequency == 0:\n",
    "            \"YOUR CODE: Sample from the replay buffer, compute the TD target, compute TD loss, and perform an optimizer step.\"\n",
    "            sample = rb.sample(args.batch_size, device)\n",
    "            s, a, r, d, s_new = sample.observations, sample.actions, sample.rewards, sample.dones, sample.next_observations\n",
    "\n",
    "            with t.inference_mode():\n",
    "                target_max = target_network(t.Tensor(s_new).to(dtype=t.float32).to(device)).max(-1).values\n",
    "            predicted_q_vals = q_network(s)[t.arange(args.batch_size), a.flatten()]\n",
    "\n",
    "            delta = r.flatten() + args.gamma * target_max * (1 - d.float().flatten()) - predicted_q_vals\n",
    "            loss = delta.pow(2).sum() / args.buffer_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            log(writer, start_time, step, predicted_q_vals, loss, infos, epsilon)\n",
    "        \n",
    "        if step % args.target_network_frequency == 0:\n",
    "            \"(4) YOUR CODE: Copy weights to the target network\"\n",
    "            target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    \"If running one of the Probe environments, will test if the learned q-values are\\n    sensible after training. Useful for debugging.\"\n",
    "    probe_batches = [\n",
    "        t.tensor([[0.0]]),\n",
    "        t.tensor([[-1.0], [+1.0]]),\n",
    "        t.tensor([[0.0], [1.0]]),\n",
    "        t.tensor([[0.0]]),\n",
    "        t.tensor([[0.0], [1.0]]),\n",
    "    ]\n",
    "    if re.match(r\"Probe(\\d)-v0\", args.env_id):\n",
    "        probe_no = int(re.match(r\"Probe(\\d)-v0\", args.env_id).group(1))\n",
    "        batch = probe_batches[probe_no]\n",
    "        value = q_network(batch)\n",
    "        print(\"Value: \", value)\n",
    "        expected = t.tensor([[1.0]]).to(device)\n",
    "        t.testing.assert_close(value, expected, 0.0001)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcurt-tigges\u001b[0m (\u001b[33marena-ldn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:46: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/curttigges/projects/arena-v1-ldn-ct/wandb/run-20221129_185454-2bst0wx0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/arena-ldn/Curt-CartPoleDQN/runs/2bst0wx0\" target=\"_blank\">CartPole-v1__DQN_implementation__1__1669748093</a></strong> to <a href=\"https://wandb.ai/arena-ldn/Curt-CartPoleDQN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/import_hooks.py:246: DeprecationWarning: Deprecated since Python 3.4. Use importlib.util.find_spec() instead.\n",
      "  loader = importlib.find_loader(fullname, path)\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/core.py:172: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=10120, episodic_return=14.0\n",
      "global_step=10470, episodic_return=13.0\n",
      "global_step=10540, episodic_return=41.0\n",
      "global_step=10640, episodic_return=13.0\n",
      "global_step=10780, episodic_return=33.0\n",
      "global_step=11010, episodic_return=9.0\n",
      "global_step=11140, episodic_return=14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=12040, episodic_return=14.0\n",
      "global_step=12310, episodic_return=16.0\n",
      "global_step=12640, episodic_return=12.0\n",
      "global_step=12950, episodic_return=22.0\n",
      "global_step=13010, episodic_return=27.0\n",
      "global_step=13130, episodic_return=11.0\n",
      "global_step=13670, episodic_return=24.0\n",
      "global_step=13770, episodic_return=12.0\n",
      "global_step=13810, episodic_return=23.0\n",
      "global_step=14030, episodic_return=27.0\n",
      "global_step=14320, episodic_return=15.0\n",
      "global_step=14400, episodic_return=32.0\n",
      "global_step=15110, episodic_return=18.0\n",
      "global_step=15470, episodic_return=17.0\n",
      "global_step=15500, episodic_return=15.0\n",
      "global_step=15590, episodic_return=18.0\n",
      "global_step=15720, episodic_return=31.0\n",
      "global_step=15810, episodic_return=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=16190, episodic_return=28.0\n",
      "global_step=16230, episodic_return=15.0\n",
      "global_step=16580, episodic_return=41.0\n",
      "global_step=16600, episodic_return=20.0\n",
      "global_step=16660, episodic_return=15.0\n",
      "global_step=16870, episodic_return=18.0\n",
      "global_step=17180, episodic_return=48.0\n",
      "global_step=17400, episodic_return=12.0\n",
      "global_step=18150, episodic_return=15.0\n",
      "global_step=19030, episodic_return=17.0\n",
      "global_step=19090, episodic_return=11.0\n",
      "global_step=19240, episodic_return=31.0\n",
      "global_step=19290, episodic_return=19.0\n",
      "global_step=19650, episodic_return=13.0\n",
      "global_step=19780, episodic_return=25.0\n",
      "global_step=19850, episodic_return=23.0\n",
      "global_step=19900, episodic_return=12.0\n",
      "SPS: 2198\n",
      "global_step=20010, episodic_return=52.0\n",
      "global_step=20300, episodic_return=31.0\n",
      "global_step=20420, episodic_return=11.0\n",
      "global_step=20610, episodic_return=16.0\n",
      "global_step=21280, episodic_return=25.0\n",
      "global_step=21430, episodic_return=16.0\n",
      "global_step=21990, episodic_return=35.0\n",
      "global_step=22440, episodic_return=13.0\n",
      "global_step=22480, episodic_return=15.0\n",
      "global_step=22610, episodic_return=21.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=22980, episodic_return=14.0\n",
      "global_step=23260, episodic_return=25.0\n",
      "global_step=23660, episodic_return=36.0\n",
      "global_step=23870, episodic_return=24.0\n",
      "global_step=23930, episodic_return=29.0\n",
      "global_step=24020, episodic_return=26.0\n",
      "global_step=24630, episodic_return=23.0\n",
      "global_step=25690, episodic_return=35.0\n",
      "global_step=25750, episodic_return=46.0\n",
      "global_step=25810, episodic_return=29.0\n",
      "global_step=25920, episodic_return=41.0\n",
      "global_step=26040, episodic_return=14.0\n",
      "global_step=26630, episodic_return=54.0\n",
      "global_step=26970, episodic_return=43.0\n",
      "global_step=27030, episodic_return=16.0\n",
      "global_step=27060, episodic_return=30.0\n",
      "global_step=27760, episodic_return=29.0\n",
      "global_step=28060, episodic_return=33.0\n",
      "global_step=28220, episodic_return=17.0\n",
      "global_step=28750, episodic_return=16.0\n",
      "global_step=29000, episodic_return=18.0\n",
      "global_step=29170, episodic_return=16.0\n",
      "global_step=29310, episodic_return=52.0\n",
      "global_step=29440, episodic_return=13.0\n",
      "global_step=29510, episodic_return=28.0\n",
      "global_step=29570, episodic_return=35.0\n",
      "global_step=29600, episodic_return=17.0\n",
      "global_step=29690, episodic_return=21.0\n",
      "global_step=29810, episodic_return=27.0\n",
      "SPS: 2155\n",
      "global_step=30090, episodic_return=23.0\n",
      "global_step=30480, episodic_return=29.0\n",
      "global_step=30600, episodic_return=36.0\n",
      "global_step=31010, episodic_return=17.0\n",
      "global_step=31580, episodic_return=29.0\n",
      "global_step=31680, episodic_return=24.0\n",
      "global_step=32020, episodic_return=11.0\n",
      "global_step=32400, episodic_return=13.0\n",
      "global_step=32490, episodic_return=29.0\n",
      "global_step=32660, episodic_return=15.0\n",
      "global_step=32740, episodic_return=29.0\n",
      "global_step=32810, episodic_return=70.0\n",
      "global_step=33020, episodic_return=53.0\n",
      "global_step=33110, episodic_return=16.0\n",
      "global_step=33780, episodic_return=29.0\n",
      "global_step=34000, episodic_return=38.0\n",
      "global_step=34060, episodic_return=16.0\n",
      "global_step=34380, episodic_return=29.0\n",
      "global_step=35000, episodic_return=32.0\n",
      "global_step=35160, episodic_return=19.0\n",
      "global_step=35240, episodic_return=14.0\n",
      "global_step=35300, episodic_return=11.0\n",
      "global_step=35340, episodic_return=40.0\n",
      "global_step=35700, episodic_return=23.0\n",
      "global_step=35840, episodic_return=26.0\n",
      "global_step=36050, episodic_return=27.0\n",
      "global_step=36850, episodic_return=25.0\n",
      "global_step=36880, episodic_return=14.0\n",
      "global_step=36950, episodic_return=24.0\n",
      "global_step=37360, episodic_return=33.0\n",
      "global_step=37660, episodic_return=48.0\n",
      "global_step=37710, episodic_return=18.0\n",
      "global_step=37970, episodic_return=16.0\n",
      "global_step=38020, episodic_return=19.0\n",
      "global_step=38170, episodic_return=42.0\n",
      "global_step=38560, episodic_return=35.0\n",
      "global_step=38890, episodic_return=24.0\n",
      "global_step=39140, episodic_return=81.0\n",
      "global_step=39580, episodic_return=21.0\n",
      "global_step=39910, episodic_return=27.0\n",
      "SPS: 2209\n",
      "global_step=40010, episodic_return=55.0\n",
      "global_step=40220, episodic_return=84.0\n",
      "global_step=40250, episodic_return=30.0\n",
      "global_step=40680, episodic_return=27.0\n",
      "global_step=41580, episodic_return=15.0\n",
      "global_step=42910, episodic_return=17.0\n",
      "global_step=43660, episodic_return=29.0\n",
      "global_step=43940, episodic_return=29.0\n",
      "global_step=44180, episodic_return=15.0\n",
      "global_step=44480, episodic_return=32.0\n",
      "global_step=44560, episodic_return=80.0\n",
      "global_step=44870, episodic_return=47.0\n",
      "global_step=45110, episodic_return=14.0\n",
      "global_step=45470, episodic_return=14.0\n",
      "global_step=45870, episodic_return=23.0\n",
      "global_step=46320, episodic_return=37.0\n",
      "global_step=46750, episodic_return=18.0\n",
      "global_step=46900, episodic_return=65.0\n",
      "global_step=46940, episodic_return=26.0\n",
      "global_step=47790, episodic_return=36.0\n",
      "global_step=47860, episodic_return=70.0\n",
      "global_step=47900, episodic_return=15.0\n",
      "global_step=47940, episodic_return=40.0\n",
      "global_step=48130, episodic_return=23.0\n",
      "global_step=48290, episodic_return=32.0\n",
      "global_step=48390, episodic_return=37.0\n",
      "global_step=49020, episodic_return=18.0\n",
      "global_step=49050, episodic_return=30.0\n",
      "global_step=49230, episodic_return=17.0\n",
      "global_step=49890, episodic_return=13.0\n",
      "SPS: 2227\n",
      "global_step=50300, episodic_return=17.0\n",
      "global_step=50580, episodic_return=24.0\n",
      "global_step=50600, episodic_return=20.0\n",
      "global_step=50630, episodic_return=30.0\n",
      "global_step=50840, episodic_return=29.0\n",
      "global_step=51010, episodic_return=31.0\n",
      "global_step=51060, episodic_return=18.0\n",
      "global_step=51270, episodic_return=93.0\n",
      "global_step=51850, episodic_return=36.0\n",
      "global_step=52050, episodic_return=19.0\n",
      "global_step=52140, episodic_return=59.0\n",
      "global_step=52450, episodic_return=39.0\n",
      "global_step=53510, episodic_return=37.0\n",
      "global_step=53640, episodic_return=16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=53930, episodic_return=17.0\n",
      "global_step=54830, episodic_return=47.0\n",
      "global_step=55000, episodic_return=73.0\n",
      "global_step=55190, episodic_return=71.0\n",
      "global_step=55670, episodic_return=11.0\n",
      "global_step=55790, episodic_return=66.0\n",
      "global_step=56310, episodic_return=15.0\n",
      "global_step=56620, episodic_return=18.0\n",
      "global_step=57490, episodic_return=74.0\n",
      "global_step=57810, episodic_return=32.0\n",
      "global_step=58160, episodic_return=29.0\n",
      "global_step=58390, episodic_return=42.0\n",
      "global_step=58460, episodic_return=48.0\n",
      "global_step=58790, episodic_return=79.0\n",
      "global_step=58870, episodic_return=31.0\n",
      "global_step=59200, episodic_return=99.0\n",
      "SPS: 2201\n",
      "global_step=60010, episodic_return=79.0\n",
      "global_step=60420, episodic_return=89.0\n",
      "global_step=60560, episodic_return=59.0\n",
      "global_step=60640, episodic_return=37.0\n",
      "global_step=60900, episodic_return=35.0\n",
      "global_step=60940, episodic_return=40.0\n",
      "global_step=61150, episodic_return=22.0\n",
      "global_step=62240, episodic_return=46.0\n",
      "global_step=62410, episodic_return=48.0\n",
      "global_step=62960, episodic_return=48.0\n",
      "global_step=63250, episodic_return=59.0\n",
      "global_step=63480, episodic_return=22.0\n",
      "global_step=63510, episodic_return=12.0\n",
      "global_step=63570, episodic_return=60.0\n",
      "global_step=64150, episodic_return=29.0\n",
      "global_step=65870, episodic_return=26.0\n",
      "global_step=66060, episodic_return=31.0\n",
      "global_step=67080, episodic_return=32.0\n",
      "global_step=67200, episodic_return=54.0\n",
      "global_step=67720, episodic_return=17.0\n",
      "global_step=68240, episodic_return=65.0\n",
      "global_step=68440, episodic_return=39.0\n",
      "global_step=68560, episodic_return=28.0\n",
      "global_step=68640, episodic_return=64.0\n",
      "global_step=69900, episodic_return=79.0\n",
      "SPS: 2175\n",
      "global_step=70540, episodic_return=55.0\n",
      "global_step=70950, episodic_return=74.0\n",
      "global_step=73470, episodic_return=84.0\n",
      "global_step=73750, episodic_return=62.0\n",
      "global_step=74600, episodic_return=81.0\n",
      "global_step=74640, episodic_return=40.0\n",
      "global_step=74730, episodic_return=36.0\n",
      "global_step=75050, episodic_return=31.0\n",
      "global_step=75080, episodic_return=30.0\n",
      "global_step=75120, episodic_return=12.0\n",
      "global_step=76330, episodic_return=32.0\n",
      "global_step=77560, episodic_return=36.0\n",
      "global_step=77600, episodic_return=40.0\n",
      "global_step=78190, episodic_return=76.0\n",
      "global_step=78390, episodic_return=63.0\n",
      "global_step=78560, episodic_return=41.0\n",
      "global_step=78880, episodic_return=104.0\n",
      "global_step=79030, episodic_return=44.0\n",
      "SPS: 2178\n",
      "global_step=80740, episodic_return=39.0\n",
      "global_step=81080, episodic_return=35.0\n",
      "global_step=81480, episodic_return=19.0\n",
      "global_step=82870, episodic_return=41.0\n",
      "global_step=83550, episodic_return=39.0\n",
      "global_step=83680, episodic_return=75.0\n",
      "global_step=83800, episodic_return=34.0\n",
      "global_step=84790, episodic_return=119.0\n",
      "global_step=85610, episodic_return=227.0\n",
      "global_step=85700, episodic_return=74.0\n",
      "global_step=85730, episodic_return=30.0\n",
      "global_step=87130, episodic_return=17.0\n",
      "global_step=87410, episodic_return=18.0\n",
      "global_step=87640, episodic_return=44.0\n",
      "global_step=88070, episodic_return=154.0\n",
      "global_step=89110, episodic_return=38.0\n",
      "global_step=89350, episodic_return=55.0\n",
      "SPS: 2195\n",
      "global_step=90730, episodic_return=111.0\n",
      "global_step=91170, episodic_return=76.0\n",
      "global_step=91320, episodic_return=24.0\n",
      "global_step=91420, episodic_return=100.0\n",
      "global_step=92450, episodic_return=31.0\n",
      "global_step=94920, episodic_return=44.0\n",
      "global_step=95540, episodic_return=95.0\n",
      "global_step=96030, episodic_return=66.0\n",
      "global_step=96350, episodic_return=62.0\n",
      "global_step=97260, episodic_return=48.0\n",
      "global_step=97680, episodic_return=88.0\n",
      "global_step=98290, episodic_return=65.0\n",
      "global_step=98900, episodic_return=16.0\n",
      "SPS: 2212\n",
      "global_step=100250, episodic_return=22.0\n",
      "global_step=100710, episodic_return=72.0\n",
      "global_step=102160, episodic_return=21.0\n",
      "global_step=102820, episodic_return=87.0\n",
      "global_step=105160, episodic_return=152.0\n",
      "global_step=106690, episodic_return=89.0\n",
      "global_step=106940, episodic_return=103.0\n",
      "global_step=107270, episodic_return=144.0\n",
      "global_step=107830, episodic_return=105.0\n",
      "global_step=108190, episodic_return=66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=109880, episodic_return=117.0\n",
      "SPS: 2199\n",
      "global_step=110530, episodic_return=48.0\n",
      "global_step=110990, episodic_return=25.0\n",
      "global_step=111970, episodic_return=65.0\n",
      "global_step=112180, episodic_return=103.0\n",
      "global_step=112420, episodic_return=101.0\n",
      "global_step=112430, episodic_return=10.0\n",
      "global_step=112630, episodic_return=200.0\n",
      "global_step=113410, episodic_return=78.0\n",
      "global_step=113940, episodic_return=94.0\n",
      "global_step=114250, episodic_return=16.0\n",
      "global_step=117080, episodic_return=49.0\n",
      "global_step=117830, episodic_return=44.0\n",
      "global_step=119390, episodic_return=45.0\n",
      "SPS: 2197\n",
      "global_step=120530, episodic_return=64.0\n",
      "global_step=120760, episodic_return=184.0\n",
      "global_step=121260, episodic_return=164.0\n",
      "global_step=121530, episodic_return=182.0\n",
      "global_step=122660, episodic_return=99.0\n",
      "global_step=123510, episodic_return=64.0\n",
      "global_step=124170, episodic_return=185.0\n",
      "global_step=124470, episodic_return=214.0\n",
      "global_step=124780, episodic_return=24.0\n",
      "global_step=125810, episodic_return=206.0\n",
      "global_step=126640, episodic_return=152.0\n",
      "global_step=128940, episodic_return=29.0\n",
      "global_step=129140, episodic_return=200.0\n",
      "SPS: 2202\n",
      "global_step=131680, episodic_return=45.0\n",
      "global_step=135370, episodic_return=189.0\n",
      "global_step=135470, episodic_return=100.0\n",
      "global_step=135510, episodic_return=40.0\n",
      "global_step=136860, episodic_return=167.0\n",
      "global_step=137050, episodic_return=190.0\n",
      "global_step=138380, episodic_return=169.0\n",
      "global_step=139300, episodic_return=43.0\n",
      "global_step=139810, episodic_return=173.0\n",
      "SPS: 2199\n",
      "global_step=140130, episodic_return=17.0\n",
      "global_step=140190, episodic_return=38.0\n",
      "global_step=140220, episodic_return=30.0\n",
      "global_step=141680, episodic_return=161.0\n",
      "global_step=142940, episodic_return=163.0\n",
      "global_step=145150, episodic_return=199.0\n",
      "global_step=145210, episodic_return=60.0\n",
      "global_step=145950, episodic_return=209.0\n",
      "global_step=146620, episodic_return=269.0\n",
      "SPS: 2198\n",
      "global_step=153970, episodic_return=208.0\n",
      "global_step=154140, episodic_return=55.0\n",
      "global_step=156280, episodic_return=308.0\n",
      "global_step=158820, episodic_return=245.0\n",
      "SPS: 2200\n",
      "global_step=161820, episodic_return=264.0\n",
      "global_step=163890, episodic_return=312.0\n",
      "SPS: 2199\n",
      "global_step=176440, episodic_return=358.0\n",
      "SPS: 2194\n",
      "global_step=181860, episodic_return=406.0\n",
      "global_step=185080, episodic_return=471.0\n",
      "global_step=185820, episodic_return=326.0\n",
      "global_step=186560, episodic_return=339.0\n",
      "global_step=187060, episodic_return=500.0\n",
      "global_step=187840, episodic_return=365.0\n",
      "global_step=188650, episodic_return=418.0\n",
      "SPS: 2181\n",
      "global_step=191750, episodic_return=457.0\n",
      "global_step=192570, episodic_return=476.0\n",
      "SPS: 2176\n",
      "global_step=202720, episodic_return=127.0\n",
      "global_step=203220, episodic_return=500.0\n",
      "SPS: 2168\n",
      "SPS: 2170\n",
      "SPS: 2170\n",
      "SPS: 2167\n",
      "SPS: 2155\n",
      "SPS: 2151\n",
      "SPS: 2149\n",
      "SPS: 2145\n",
      "SPS: 2143\n",
      "SPS: 2141\n",
      "SPS: 2139\n",
      "SPS: 2131\n",
      "SPS: 2128\n",
      "SPS: 2125\n",
      "SPS: 2118\n",
      "SPS: 2112\n",
      "SPS: 2104\n",
      "SPS: 2100\n",
      "SPS: 2088\n",
      "global_step=390810, episodic_return=434.0\n",
      "global_step=392610, episodic_return=389.0\n",
      "global_step=398700, episodic_return=413.0\n",
      "global_step=399110, episodic_return=410.0\n",
      "SPS: 2070\n",
      "SPS: 2056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:115: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"video.frames_per_second\"] is marked as deprecated and will be replaced with `env.metadata[\"render_fps\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if distutils.version.LooseVersion(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPS: 2021\n",
      "global_step=421880, episodic_return=424.0\n",
      "SPS: 2005\n",
      "global_step=435740, episodic_return=206.0\n",
      "SPS: 1989\n",
      "SPS: 1973\n",
      "SPS: 1965\n",
      "SPS: 1957\n",
      "SPS: 1952\n",
      "SPS: 1945\n"
     ]
    }
   ],
   "source": [
    "train_dqn(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(\n",
    "            f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\"\n",
    "        )\n",
    "        args = DQNArgs()\n",
    "    else:\n",
    "        args = parse_args()\n",
    "    # train_dqn(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
