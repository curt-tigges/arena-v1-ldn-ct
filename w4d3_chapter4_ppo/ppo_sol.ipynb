{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from einops import rearrange\n",
    "\n",
    "from utils import make_env, ppo_parse_args\n",
    "import tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from distutils.util import strtobool\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "from einops import rearrange\n",
    "from utils import ppo_parse_args, make_env\n",
    "import part4_dqn_solution\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "RUNNING_FROM_FILE = \"ipykernel_launcher\" in os.path.basename(sys.argv[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    \"\"\"Performs orthogonal initialization on a Pytorch layer\n",
    "\n",
    "    Args:\n",
    "        layer (nn.Module): Layer module to be initialized\n",
    "        std (float, optional): Scaling factor to be used for init. Defaults to np.sqrt(2).\n",
    "        bias_const (float, optional): Constant value to fill bias. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Initialized layer module.\n",
    "    \"\"\"    \n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    critic: nn.Sequential\n",
    "    actor: nn.Sequential\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv):\n",
    "        super().__init__()\n",
    "        self.obs_shape = envs.single_observation_space.shape\n",
    "        self.n_obs = np.array(self.obs_shape).item()\n",
    "        self.n_actions = envs.single_action_space.n\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_obs, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, self.n_actions), std=0.01),\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_obs, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=0.01),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(\n",
    "    next_value: t.Tensor,\n",
    "    next_done: t.Tensor,\n",
    "    rewards: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    dones: t.Tensor,\n",
    "    device: t.device,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Compute advantages using Generalized Advantage Estimation.\n",
    "\n",
    "    next_value: shape (1, env) - represents V(s_{t+1}) which is needed for the last advantage term\n",
    "    next_done: shape (env,)\n",
    "    rewards: shape (t, env)\n",
    "    values: shape (t, env)\n",
    "    dones: shape (t, env)\n",
    "\n",
    "    Return: shape (t, env)\n",
    "    \"\"\"\n",
    "    T = values.shape[0]\n",
    "    next_values = torch.concat([values[1:], next_value])\n",
    "    next_dones = torch.concat([dones[1:], next_done.unsqueeze(0)])\n",
    "    deltas = rewards + gamma * next_values * (1.0 - next_dones) - values\n",
    "\n",
    "    advantages = deltas.clone().to(device)\n",
    "    for t in reversed(range(1, T)):\n",
    "        advantages[t - 1] = (\n",
    "            deltas[t - 1] + gamma * gae_lambda * (1.0 - dones[t]) * advantages[t]\n",
    "        )\n",
    "    return advantages\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_compute_advantages(compute_advantages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_minibatch_indexes` passed.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Minibatch:\n",
    "    obs: t.Tensor\n",
    "    logprobs: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    advantages: t.Tensor\n",
    "    returns: t.Tensor\n",
    "    values: t.Tensor\n",
    "\n",
    "\n",
    "def minibatch_indexes(batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n",
    "    \"\"\"Return a list of length (batch_size // minibatch_size) where each element is an\n",
    "    array of indexes into the batch.\n",
    "\n",
    "    Each index should appear exactly once.\n",
    "    \"\"\"\n",
    "    assert batch_size % minibatch_size == 0\n",
    "    minibatch_count = batch_size // minibatch_size\n",
    "    all_idx = np.random.permutation(batch_size)\n",
    "    return list(rearrange(\n",
    "        all_idx,\n",
    "        \"(minibatch_count minibatch_size) -> minibatch_count minibatch_size\",\n",
    "        minibatch_count=minibatch_count,\n",
    "    ))\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_minibatch_indexes(minibatch_indexes)\n",
    "\n",
    "\n",
    "def make_minibatches(\n",
    "    obs: t.Tensor,\n",
    "    logprobs: t.Tensor,\n",
    "    actions: t.Tensor,\n",
    "    advantages: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    obs_shape: tuple,\n",
    "    action_shape: tuple,\n",
    "    batch_size: int,\n",
    "    minibatch_size: int,\n",
    ") -> list[Minibatch]:\n",
    "    \"\"\"Flatten the environment and steps dimension into one batch dimension, then\n",
    "    shuffle and split into minibatches.\"\"\"\n",
    "    returns = advantages + values\n",
    "\n",
    "    data = (obs, logprobs, actions, advantages, returns, values)\n",
    "    shapes = (obs_shape, (), action_shape, (), (), ())\n",
    "    return [\n",
    "        Minibatch(*[d.reshape((-1,) + s)[ind] for d, s in zip(data, shapes)])\n",
    "        for ind in minibatch_indexes(batch_size, minibatch_size)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_policy_loss` passed.\n"
     ]
    }
   ],
   "source": [
    "def calc_policy_loss(\n",
    "    probs: Categorical,\n",
    "    mb_action: t.Tensor,\n",
    "    mb_advantages: t.Tensor,\n",
    "    mb_logprobs: t.Tensor,\n",
    "    clip_coef: float,\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Return the policy loss, suitable for maximisation with gradient ascent.\n",
    "\n",
    "    Args:\n",
    "        probs (Categorical): a distribution containing the actor's unnormalized logits\n",
    "            of shape (minibatch, num_actions)\n",
    "        mb_action (t.Tensor): Action probabilities from current theta\n",
    "        mb_advantages (t.Tensor): Advantages\n",
    "        mb_logprobs (t.Tensor): Log probabilities from previous theta\n",
    "        clip_coef (float): amount of clipping, denoted by epsilon in Eq 7.\n",
    "\n",
    "    Returns:\n",
    "        t.Tensor: Loss from minimum of unclipped and clipped loss.\n",
    "    \"\"\"\n",
    "    # perform subtraction in logspace - equivalent to divsion\n",
    "    logspace_r = probs.log_prob(mb_action) - mb_logprobs\n",
    "    ratio = t.exp(logspace_r)  # exiting logspace\n",
    "\n",
    "    # normalized advantage\n",
    "    A = (mb_advantages - mb_advantages.mean()) / mb_advantages.std()\n",
    "\n",
    "    # finalize L CLIP\n",
    "    left = ratio * A\n",
    "    right = t.clamp(ratio, 1 - clip_coef, 1 + clip_coef) * A\n",
    "    return t.minimum(left, right).mean()\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_policy_loss(calc_policy_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_value_function_loss` passed!\n"
     ]
    }
   ],
   "source": [
    "def calc_value_function_loss(\n",
    "    critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float\n",
    ") -> t.Tensor:\n",
    "    \"\"\"Computes the value function portion of the loss function\n",
    "\n",
    "    Args:\n",
    "        critic (nn.Sequential): Critic network\n",
    "        mb_obs (t.Tensor): Observations from minibatch\n",
    "        mb_returns (t.Tensor): Observed returns\n",
    "        v_coef (float): the coefficient for the value loss, which weights its\n",
    "            contribution to the overall loss. Denoted by c_1 in the paper.\n",
    "\n",
    "    Returns:\n",
    "        t.Tensor: Loss for the critic network\n",
    "    \"\"\"\n",
    "    preds = critic(mb_obs)\n",
    "    loss = ((preds - mb_returns) ** 2).mean() / 2\n",
    "    return v_coef * loss\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_value_function_loss(calc_value_function_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n",
    "    \"\"\"Return the entropy loss term\n",
    "\n",
    "    Args:\n",
    "        probs (Categorical): a distribution containing the actor's unnormalized logits\n",
    "            of shape (minibatch, num_actions)\n",
    "        ent_coef (float): the coefficient for the entropy loss, which weights its \n",
    "            contribution to the overall loss. Denoted by c_2 in the paper.\n",
    "    \"\"\"\n",
    "    return ent_coef * probs.entropy().mean()\n",
    "\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_entropy_loss(calc_entropy_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    \"\"\"Learning rate scheduler for PPO.\n",
    "    \"\"\"    \n",
    "    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): An Optimizer whose \"lr\" will be scheduled\n",
    "            initial_lr (float): Initial learning rate\n",
    "            end_lr (float): Final learning rate\n",
    "            num_updates (int): Number of steps in which the learning rate is updated\n",
    "        \"\"\"        \n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_updates = num_updates\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Performs linear learning rate decay so that after num_updates calls to step, \n",
    "        the learning rate is end_lr.\"\"\"\n",
    "        self.n_step_calls += 1\n",
    "        slope = self.n_step_calls / self.num_updates\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group[\"lr\"] = self.initial_lr + slope * (self.end_lr - self.initial_lr)\n",
    "\n",
    "\n",
    "def make_optimizer(\n",
    "    agent: Agent, num_updates: int, initial_lr: float, end_lr: float\n",
    ") -> tuple[optim.Adam, PPOScheduler]:\n",
    "    \"\"\"Return an appropriately configured Adam with its attached scheduler.\n",
    "\n",
    "    Args:\n",
    "        agent (Agent): Agent network\n",
    "        num_updates (int): Number of steps in which the learning rate is updated\n",
    "        initial_lr (float): Initial learning rate\n",
    "        end_lr (float): Final learning rate\n",
    "\n",
    "    Returns:\n",
    "        tuple[optim.Adam, PPOScheduler]: Optimizer and scheduler objects\n",
    "    \"\"\"\n",
    "    optimizer = t.optim.Adam(agent.parameters(), initial_lr, eps=1e-5,maximize=True)\n",
    "    scheduler = PPOScheduler(optimizer, initial_lr, end_lr, num_updates)\n",
    "    return optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPOArgs:\n",
    "    exp_name: str = os.path.basename(\"ppo_sol\").rstrip(\".py\")\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"Curt-PPOCart\"\n",
    "    wandb_entity: str = None\n",
    "    capture_video: bool = True\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    batch_size: int = 512\n",
    "    minibatch_size: int = 128\n",
    "\n",
    "\n",
    "def train_ppo(args: PPOArgs):\n",
    "\n",
    "    # Set up tracking and reporting\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\"\n",
    "        % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    )\n",
    "\n",
    "    # Set initial settings\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # Initialize environments, action shapes and agent\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [\n",
    "            make_env(args.env_id, args.seed + i, i, args.capture_video, run_name)\n",
    "            for i in range(args.num_envs)\n",
    "        ]\n",
    "    )\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    assert action_shape is not None\n",
    "    assert isinstance(\n",
    "        envs.single_action_space, Discrete\n",
    "    ), \"only discrete action space is supported\"\n",
    "    agent = Agent(envs).to(device)\n",
    "\n",
    "    # Set up training loop components and values to be used for optimization\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n",
    "    obs = torch.zeros(\n",
    "        (args.num_steps, args.num_envs) + envs.single_observation_space.shape\n",
    "    ).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    global_step = 0\n",
    "    old_approx_kl = 0.0\n",
    "    approx_kl = 0.0\n",
    "    value_loss = t.tensor(0.0)\n",
    "    policy_loss = t.tensor(0.0)\n",
    "    entropy_loss = t.tensor(0.0)\n",
    "    clipfracs = []\n",
    "    info = []\n",
    "\n",
    "    # Initialize observations\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    \n",
    "    # Begin training loop\n",
    "    for _ in range(num_updates):\n",
    "\n",
    "        # Rollout loop\n",
    "        for i in range(0, args.num_steps):\n",
    "            # Rollout calculations\n",
    "            obs[i] = next_obs\n",
    "            dones[i] = next_done\n",
    "\n",
    "            with t.inference_mode():\n",
    "                next_values = agent.critic(next_obs).flatten()\n",
    "                logits = agent.actor(next_obs)\n",
    "            probs = Categorical(logits=logits)\n",
    "            action = probs.sample()\n",
    "            logprob = probs.log_prob(action)\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            \n",
    "            rewards[i] = t.from_numpy(reward)\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            values[i] = next_values\n",
    "\n",
    "            next_obs = t.from_numpy(next_obs).to(device)\n",
    "            next_done = t.from_numpy(done).float().to(device)\n",
    "\n",
    "            # Reporting\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(\n",
    "                        f\"global_step={global_step}, episodic_return={item['episode']['r']}\"\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        \"charts/episodic_return\", item[\"episode\"][\"r\"], global_step\n",
    "                    )\n",
    "                    writer.add_scalar(\n",
    "                        \"charts/episodic_length\", item[\"episode\"][\"l\"], global_step\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        # Get the next value and advantages\n",
    "        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n",
    "        advantages = compute_advantages(\n",
    "            next_value,\n",
    "            next_done,\n",
    "            rewards,\n",
    "            values,\n",
    "            dones,\n",
    "            device,\n",
    "            args.gamma,\n",
    "            args.gae_lambda,\n",
    "        )\n",
    "        clipfracs.clear()\n",
    "\n",
    "        # Learning loop\n",
    "        for _ in range(args.update_epochs):\n",
    "            minibatches = make_minibatches(\n",
    "                obs,\n",
    "                logprobs,\n",
    "                actions,\n",
    "                advantages,\n",
    "                values,\n",
    "                envs.single_observation_space.shape,\n",
    "                action_shape,\n",
    "                args.batch_size,\n",
    "                args.minibatch_size,\n",
    "            )\n",
    "            for mb in minibatches:\n",
    "                # YOUR CODE: compute loss on the minibatch and step the optimizer (not \n",
    "                # the scheduler). Do detail #11 (global gradient clipping) here using \n",
    "                # nn.utils.clip_grad_norm_.\n",
    "                logits = agent.actor(mb.obs)\n",
    "                probs = Categorical(logits=logits)\n",
    "                policy_loss = calc_policy_loss(probs, mb.actions, mb.advantages, mb.logprobs, args.clip_coef)\n",
    "                value_loss = calc_value_function_loss(agent.critic, mb.obs, mb.returns, args.vf_coef)\n",
    "                entropy_loss = calc_entropy_loss(probs, args.ent_coef)\n",
    "                loss = policy_loss - value_loss + entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Step scheduler, calculate metrics\n",
    "        scheduler.step()\n",
    "        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        with torch.no_grad():\n",
    "            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n",
    "            logratio = newlogprob - mb.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            old_approx_kl = (-logratio).mean().item()\n",
    "            approx_kl = (ratio - 1 - logratio).mean().item()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        \n",
    "        \n",
    "        # Add metrics to reporting\n",
    "        writer.add_scalar(\n",
    "            \"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step\n",
    "        )\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\n",
    "            \"charts/SPS\", int(global_step / (time.time() - start_time)), global_step\n",
    "        )\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"steps per second (SPS):\", int(global_step / (time.time() - start_time))\n",
    "            )\n",
    "        wandb.log({\n",
    "            \"value_loss\": value_loss, \"policy_loss\":policy_loss,\n",
    "            \"entropy\":entropy_loss, \"old_approx_kl\":old_approx_kl, \"approx_kl\":approx_kl,\n",
    "            \"clipfrac\":np.mean(clipfracs), \"explained_variance\":explained_var})\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# if MAIN:\n",
    "#     if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "#         filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "#         print(\n",
    "#             f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\"\n",
    "#         )\n",
    "#         args = PPOArgs()\n",
    "#     else:\n",
    "#         args = ppo_parse_args()\n",
    "#     train_ppo(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment EasyCart-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "import math\n",
    "\n",
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        (obs, rew, done, info) = super().step(action)\n",
    "        \"YOUR CODE HERE\"\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:46: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o7d1ql4y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o7d1ql4y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:58: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320aa287b5d8480f919dc26a84818593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668982865909735, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n",
      "    self._process(record)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/internal.py\", line 308, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/sender.py\", line 305, in send\n",
      "    send_handler(record)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/sender.py\", line 319, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/sender.py\", line 491, in send_request_defer\n",
      "    transition_state()\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/internal/sender.py\", line 464, in transition_state\n",
      "    self._interface.publish_defer(state)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 285, in publish_defer\n",
      "    self._publish_defer(cast(\"pb.DeferRequest.DeferState.V\", state))\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 282, in _publish_defer\n",
      "    self._publish(rec, local=True)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/interface/interface_queue.py\", line 59, in _publish\n",
      "    self.record_q.put(record)\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/multiprocessing/queues.py\", line 88, in put\n",
      "    raise ValueError(f\"Queue {self!r} is closed\")\n",
      "ValueError: Queue <multiprocessing.queues.Queue object at 0x7f788ddad3a0> is closed\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_1416327/3643760415.py 35 train_ppo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 1078, in init\n",
      "    run = wi.init()\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/wandb_init.py\", line 697, in init\n",
      "    result = handle.wait(\n",
      "  File \"/home/curttigges/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py\", line 259, in wait\n",
      "    raise MailboxError(\"transport failed\")\n",
      "wandb.errors.MailboxError: transport failed\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "problem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMailboxError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1078\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1078\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1079\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n",
      "File \u001b[0;32m~/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:697\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 697\u001b[0m result \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    698\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings\u001b[39m.\u001b[39;49minit_timeout, on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init\n\u001b[1;32m    699\u001b[0m )\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:259\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m--> 259\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slot\u001b[39m.\u001b[39m_get_and_clear(timeout\u001b[39m=\u001b[39mwait_timeout)\n",
      "\u001b[0;31mMailboxError\u001b[0m: transport failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m PPOArgs()\n\u001b[0;32m----> 2\u001b[0m train_ppo(args)\n",
      "Cell \u001b[0;32mIn [12], line 35\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mtrack:\n\u001b[1;32m     33\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     wandb\u001b[39m.\u001b[39;49minit(\n\u001b[1;32m     36\u001b[0m         project\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mwandb_project_name,\n\u001b[1;32m     37\u001b[0m         entity\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mwandb_entity,\n\u001b[1;32m     38\u001b[0m         sync_tensorboard\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mvars\u001b[39;49m(args),\n\u001b[1;32m     40\u001b[0m         name\u001b[39m=\u001b[39;49mrun_name,\n\u001b[1;32m     41\u001b[0m         monitor_gym\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     42\u001b[0m         save_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m writer \u001b[39m=\u001b[39m SummaryWriter(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mruns/\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m writer\u001b[39m.\u001b[39madd_text(\n\u001b[1;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhyperparameters\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m|param|value|\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m|-|-|\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m     \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m (key, value) \u001b[39min\u001b[39;00m \u001b[39mvars\u001b[39m(args)\u001b[39m.\u001b[39mitems()]),\n\u001b[1;32m     49\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/arena/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1116\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[39mif\u001b[39;00m except_exit:\n\u001b[1;32m   1115\u001b[0m             os\u001b[39m.\u001b[39m_exit(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 1116\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mproblem\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merror_seen\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mException\u001b[0m: problem"
     ]
    }
   ],
   "source": [
    "args = PPOArgs()\n",
    "train_ppo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
