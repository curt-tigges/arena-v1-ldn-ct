{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "import sys \n",
    "sys.path.append('../tests')\n",
    "\n",
    "import test_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class LAMBSimple:\n",
    "\n",
    "    def __init__(\n",
    "        self, params: Iterable[t.nn.parameter.Parameter], lr: float,\n",
    "        betas: tuple[float, float], eps: float, weight_decay: float):\n",
    "        '''Implements Layer-wise Adaptive Moments optimizer for Batch training.\n",
    "        \n",
    "        Accepts parameter iterables.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        self.eta = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.lam = weight_decay\n",
    "        self.thetas = list(params)\n",
    "        #self.phi = scale_func\n",
    "        self.previous_m_t = [t.zeros_like(p) for p in self.thetas]\n",
    "        self.previous_v_t = [t.zeros_like(p) for p in self.thetas]\n",
    "        self.t = 1\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.thetas:\n",
    "            param.grad = t.zeros_like(param)\n",
    "\n",
    "    #@t.inference_mode()\n",
    "    #def lamb(self, param_group):\n",
    "    #    pass\n",
    "    \n",
    "    @t.inference_mode()\n",
    "    def step(self):\n",
    "        # each \"param\" represents a matrix of the parameters in a layer\n",
    "        for i, param in enumerate(self.thetas):\n",
    "            # maximize=false, so:\n",
    "            g_t = param.grad\n",
    "\n",
    "            m_t = self.beta1 * self.previous_m_t[i] + (1 - self.beta1) * g_t\n",
    "            v_t = self.beta2 * self.previous_v_t[i] + (1 - self.beta2) * g_t ** 2\n",
    "\n",
    "            m_t = m_t / (1 - self.beta1 ** self.t)\n",
    "            v_t = v_t / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            r_t = m_t / (v_t ** 0.5 + self.eps)\n",
    "\n",
    "            #print(f\"param shape: {param.shape} m_t shape: {m_t.shape} v_t shape: {v_t.shape} r_t shape: {r_t.shape}\")\n",
    "\n",
    "            param_norm = t.linalg.norm(param.detach(), dim=None, ord=2)\n",
    "            #print(f\"param_norm shape: {param_norm.shape}\")\n",
    "\n",
    "            if self.lam != 0:\n",
    "                param = self.lam * param\n",
    "                #print(f\"param shape after wd: {param.shape}\")\n",
    "\n",
    "            update_norm = t.linalg.norm((r_t + param), dim=None, ord=2)\n",
    "\n",
    "            #print(f\"update_norm shape: {update_norm.shape}\")\n",
    "            #print(f\"param_norm: {param_norm} update_norm: {update_norm}\")\n",
    "\n",
    "            r = param_norm / update_norm\n",
    "\n",
    "            #print(f\"r shape: {r.shape}\")\n",
    "\n",
    "            eta = r * self.eta\n",
    "            \n",
    "            #print(f\"eta shape: {eta.shape}\")\n",
    "            param -= eta * (r_t + param)\n",
    "            \n",
    "            self.previous_m_t[i] = m_t\n",
    "            self.previous_v_t[i] = v_t\n",
    "        self.t += 1\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        # Should return something reasonable here, e.g. \"SGD(lr=lr, ...)\"\n",
    "        return f\"lr={self.gamma}, momentum={self.mu}, weight_decay={self.lam}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing configuration:  {'lr': 0.1, 'betas': (0.8, 0.95), 'eps': 0.001, 'weight_decay': 0.0}\n",
      "actual: Parameter containing:\n",
      "tensor([[ 0.0476,  0.6076],\n",
      "        [-0.7025,  0.3038],\n",
      "        [-0.0951,  0.2789],\n",
      "        [ 0.6028, -0.1554],\n",
      "        [ 0.2378,  0.8149],\n",
      "        [ 0.3952,  0.7227],\n",
      "        [-0.4194,  0.5829],\n",
      "        [ 0.7097, -0.2370],\n",
      "        [ 0.0316, -0.6797],\n",
      "        [ 0.4186, -0.7241],\n",
      "        [-0.3617,  0.6916],\n",
      "        [ 0.0441,  0.1000],\n",
      "        [ 0.5072, -0.0614],\n",
      "        [ 0.7546, -0.2747],\n",
      "        [-0.7075, -0.3306],\n",
      "        [ 0.0702,  0.2893],\n",
      "        [-0.3142, -0.5099],\n",
      "        [ 0.6492,  0.6795],\n",
      "        [ 0.6989,  0.7466],\n",
      "        [ 0.0683, -0.1927],\n",
      "        [-0.2699, -0.7493],\n",
      "        [ 0.3154, -0.3336],\n",
      "        [-0.6454,  0.4441],\n",
      "        [-0.0249,  0.0024],\n",
      "        [ 0.1194,  0.0896],\n",
      "        [ 0.8464, -0.0952],\n",
      "        [ 0.4026, -0.0296],\n",
      "        [-0.5598,  0.0045],\n",
      "        [ 0.4658, -0.8837],\n",
      "        [-0.2521,  0.3666],\n",
      "        [ 0.1568,  0.3738],\n",
      "        [-0.8584,  0.9314]], requires_grad=True)\n",
      "submitted: Parameter containing:\n",
      "tensor([[-0.0152,  0.3353],\n",
      "        [-0.5430,  0.1403],\n",
      "        [-0.0742,  0.2176],\n",
      "        [ 0.3888, -0.1244],\n",
      "        [ 0.1877,  0.5938],\n",
      "        [ 0.3452,  0.5190],\n",
      "        [-0.2973,  0.3167],\n",
      "        [ 0.4721, -0.1469],\n",
      "        [ 0.0353, -0.5753],\n",
      "        [ 0.2739, -0.5832],\n",
      "        [-0.2028,  0.4181],\n",
      "        [ 0.0344,  0.0780],\n",
      "        [ 0.3287,  0.0524],\n",
      "        [ 0.6391, -0.1363],\n",
      "        [-0.5999, -0.1917],\n",
      "        [ 0.1345,  0.1448],\n",
      "        [-0.2459, -0.3519],\n",
      "        [ 0.5405,  0.3885],\n",
      "        [ 0.5462,  0.5389],\n",
      "        [ 0.0533, -0.1504],\n",
      "        [-0.2419, -0.5815],\n",
      "        [ 0.2652, -0.2395],\n",
      "        [-0.4539,  0.2664],\n",
      "        [ 0.0126,  0.0484],\n",
      "        [ 0.0932,  0.0699],\n",
      "        [ 0.5925, -0.0706],\n",
      "        [ 0.2889, -0.0543],\n",
      "        [-0.3783, -0.0549],\n",
      "        [ 0.2663, -0.6740],\n",
      "        [-0.1439,  0.2509],\n",
      "        [ 0.1380,  0.2288],\n",
      "        [-0.6389,  0.6741]], requires_grad=True)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 64 / 64 (100.0%)\nGreatest absolute difference: 0.29106128215789795 at index (17, 1) (up to 1e-05 allowed)\nGreatest relative difference: 4.133229074600336 at index (0, 0) (up to 0 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_optimizers\u001b[39m.\u001b[39;49mtest_lamb(LAMBSimple)\n",
      "File \u001b[0;32m~/projects/arena-v1-ldn-ct/common_modules/experimental_notebooks/../tests/test_optimizers.py:94\u001b[0m, in \u001b[0;36mtest_lamb\u001b[0;34m(LAMBSimple)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual: \u001b[39m\u001b[39m{\u001b[39;00mw0_correct\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msubmitted: \u001b[39m\u001b[39m{\u001b[39;00mw0_submitted\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m t\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_close(w0_correct, w0_submitted, rtol\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, atol\u001b[39m=\u001b[39;49m\u001b[39m1e-5\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/arena/lib/python3.9/site-packages/torch/testing/_comparison.py:1093\u001b[0m, in \u001b[0;36massert_equal\u001b[0;34m(actual, expected, pair_types, sequence_types, mapping_types, msg, **options)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[39m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1093\u001b[0m \u001b[39mraise\u001b[39;00m error_metas[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 64 / 64 (100.0%)\nGreatest absolute difference: 0.29106128215789795 at index (17, 1) (up to 1e-05 allowed)\nGreatest relative difference: 4.133229074600336 at index (0, 0) (up to 0 allowed)"
     ]
    }
   ],
   "source": [
    "test_optimizers.test_lamb(LAMBSimple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
