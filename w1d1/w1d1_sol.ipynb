{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"\"\"A day will come, one day in the unending succession of days, \n",
    "    when beings, beings who are now latent in our thoughts and hidden \n",
    "    in our loins, shall stand upon this earth as one stands upon a \n",
    "    footstool, and shall laugh and reach out their hands amidst the \n",
    "    stars.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkns = tokenizer.encode(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A day will come, one day in the unending succession of days, \\n    when beings, beings who are now latent in our thoughts and hidden \\n    in our loins, shall stand upon this earth as one stands upon a \\n    footstool, and shall laugh and reach out their hands amidst the \\n    stars.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tkns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32, 1110, 481, 1282, 11, 530, 1110, 287, 262, 555, 1571, 22435, 286, 1528, 11, 220, 198, 220, 220, 220, 618, 9791, 11, 9791, 508, 389, 783, 41270, 287, 674, 6066, 290, 7104, 220, 198, 220, 220, 220, 287, 674, 2376, 1040, 11, 2236, 1302, 2402, 428, 4534, 355, 530, 6296, 2402, 257, 220, 198, 220, 220, 220, 2366, 301, 970, 11, 290, 2236, 6487, 290, 3151, 503, 511, 2832, 31095, 262, 220, 198, 220, 220, 220, 5788, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_embed = num_embeddings\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.weight = nn.Parameter(t.ones(num_embeddings, embedding_dim).uniform_(-1, to=1))\n",
    "\n",
    "    def forward(self, x: t.LongTensor) -> t.Tensor:\n",
    "        '''For each integer in the input, return that row of the embedding.\n",
    "        '''\n",
    "        return self.weight[x]\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"{self.num_embed}, {self.embed_dim}\"\n",
    "\n",
    "assert repr(Embedding(10, 20)) == repr(t.nn.Embedding(10, 20))\n",
    "#utils.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_seq_len: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embedding_dim\n",
    "        self.n = 10000\n",
    "        \n",
    "        freqs = np.outer(\n",
    "            np.arange(max_seq_len), \n",
    "            1 / self.n ** (2 * np.arange(embedding_dim//2) / embedding_dim)\n",
    "        )\n",
    "        enc_2d = np.zeros((max_seq_len, embedding_dim))\n",
    "        enc_2d[:, ::2] = np.sin(freqs)\n",
    "        enc_2d[:, 1::2] = np.cos(freqs)\n",
    "        self.pos_enc = t.from_numpy(enc_2d)\n",
    "        self.register_buffer(\"pos_enc\", self.pos_enc)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq_len, embedding_dim)\n",
    "        '''\n",
    "        return x + self.pos_enc[:x.shape[1],:]\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"max_freq={self.n}, max_seq_len={self.max_seq_len}, embedding_dim={self.embed_dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = t.randn(2, 3, 4)\n",
    "lnorm = nn.LayerNorm(T.shape[2])\n",
    "out = lnorm(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0023,  1.4372,  2.0048, -0.3291],\n",
       "         [-0.6571,  1.2354, -0.8084,  1.0779],\n",
       "         [ 0.3791,  1.1162, -0.5283,  1.3747]],\n",
       "\n",
       "        [[ 2.1198, -0.0561, -0.4570,  0.9201],\n",
       "         [ 0.1503,  1.6929, -0.4923,  0.3095],\n",
       "         [-1.3740, -0.8491,  1.3189,  1.8742]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2405,  0.7375,  1.1977, -0.6947],\n",
       "         [-0.9169,  1.0798, -1.0764,  0.9136],\n",
       "         [-0.2790,  0.7177, -1.5060,  1.0673]],\n",
       "\n",
       "        [[ 1.4964, -0.6916, -1.0947,  0.2900],\n",
       "         [-0.3325,  1.6043, -1.1392, -0.1326],\n",
       "         [-1.1707, -0.7906,  0.7796,  1.1817]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_layernorm_mean_1d` passed.\n",
      "All tests in `test_layernorm_mean_2d` passed.\n",
      "All tests in `test_layernorm_std` passed.\n",
      "All tests in `test_layernorm_exact` passed.\n",
      "All tests in `test_layernorm_backward` passed.\n"
     ]
    }
   ],
   "source": [
    "#from types import UnionType  # type: ignore\n",
    "#from typing import List  # type: ignore\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        normalized_shape, \n",
    "        eps: float = 1e-05, \n",
    "        elementwise_affine: bool = True\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.norm_shape = (normalized_shape, ) if isinstance(normalized_shape, int) else normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.weight = nn.Parameter(t.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(t.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''Normalize along each embedding'''\n",
    "        x_dims, norm_shape_dims = len(x.shape), len(self.norm_shape)\n",
    "        norm_dims = tuple([d for d in range(x_dims - norm_shape_dims, x_dims)])\n",
    "        \n",
    "        self.mean = t.mean(x, dim=norm_dims, keepdim=True)\n",
    "        self.var = t.var(x, dim=norm_dims, unbiased=False, keepdim=True)\n",
    "\n",
    "        out = (x - self.mean) / t.sqrt(self.var + self.eps)\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            out = out * self.weight + self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"normalized_shape={self.norm_shape}, eps={self.eps}, elementwise_affine={self.elementwise_affine}\"\n",
    "\n",
    "utils.test_layernorm_mean_1d(LayerNorm)\n",
    "utils.test_layernorm_mean_2d(LayerNorm)\n",
    "utils.test_layernorm_std(LayerNorm)\n",
    "utils.test_layernorm_exact(LayerNorm)\n",
    "utils.test_layernorm_backward(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        pass\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        pass\n",
    "\n",
    "utils.test_dropout_eval(Dropout)\n",
    "utils.test_dropout_training(Dropout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
