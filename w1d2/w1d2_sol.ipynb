{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import utils\n",
    "import cnn_modules as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, embed_size)\n",
    "    K: shape (batch, seq_len, embed_size)\n",
    "    V: shape (batch, seq_len, embed_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, embed_size)\n",
    "    '''\n",
    "    scores = Q @ t.transpose(K, -2, -1)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B Seq Score, B Seq Emb -> B Seq Emb', scores, V)\n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of masked self-attention.\n",
    "\n",
    "    See \"The Decoder Side\" section of the Illustrated Transformer for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, embed_size)\n",
    "    K: shape (batch, seq_len, embed_size)\n",
    "    V: shape (batch, seq_len, embed_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, embed_size)\n",
    "    '''\n",
    "    batch_size, seq_len, embed_size = Q.shape\n",
    "    scores = Q @ t.transpose(K, -2, -1)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "    \n",
    "    # create lower-left triangle of ones, including the diagonal\n",
    "    mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "    # fill with close-to-neg-inf values\n",
    "    scores = scores.masked_fill(mask==0, -1e9)\n",
    "    \n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B Seq Score, B Seq Emb -> B Seq Emb', scores, V)\n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*headsize)\n",
    "    K: shape (batch, seq, nheads*headsize)\n",
    "    V: shape (batch, seq, nheads*headsize)\n",
    "\n",
    "    returns: shape (batch, seq, nheads*headsize)\n",
    "    '''\n",
    "    Q = rearrange(Q, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "    K = rearrange(K, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "    V = rearrange(V, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "\n",
    "    batch_size, seq_len, nheads, headsize = Q.shape\n",
    "    scores = einsum('B Qseq nheads headsize, B Kseq nheads headsize -> B nheads Qseq Kseq', Q, K)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "\n",
    "    # create lower-left triangle of ones, including the diagonal\n",
    "    mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "    # fill with close-to-neg-inf values where mask==0\n",
    "    scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B nheads Qseq Kseq, B Qseq nheads headsize -> B Qseq nheads headsize', scores, V)\n",
    "    Z = rearrange(Z, 'B Qseq nheads headsize -> B Qseq (nheads headsize)')\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = t.randn(2, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8941, -0.4399,  0.2117,  2.8757],\n",
       "         [ 0.6109, -0.6800, -0.3416, -2.5958],\n",
       "         [-0.6437,  3.2413, -0.4565,  0.2686],\n",
       "         [ 0.0507,  0.1856,  0.2923,  1.2261],\n",
       "         [-0.6602, -0.6553,  0.2737, -1.5608],\n",
       "         [-0.0852,  1.3946,  0.0421,  0.4518],\n",
       "         [ 0.5860, -2.2522, -1.8987, -0.4523],\n",
       "         [-0.4501,  0.5583,  0.0848,  1.6969],\n",
       "         [-0.0303,  0.1286, -0.8724,  1.1841],\n",
       "         [ 0.6496,  0.1996, -0.5088, -0.4013]],\n",
       "\n",
       "        [[-0.0852,  0.0814,  0.2323,  1.2971],\n",
       "         [ 0.5716,  0.2464,  0.3229,  0.2581],\n",
       "         [ 0.8881,  1.0325, -1.0889, -2.5788],\n",
       "         [ 1.8355, -0.1379, -0.8073,  0.6323],\n",
       "         [ 0.0549, -0.2533, -1.2703,  0.9032],\n",
       "         [ 0.4287, -0.3284, -1.0406,  0.1846],\n",
       "         [ 0.9691, -0.3803,  0.2693,  0.8820],\n",
       "         [-0.0184,  1.3782, -0.5900, -0.8977],\n",
       "         [ 0.6414,  0.9656,  0.2465,  1.4997],\n",
       "         [ 0.2221, -1.4293, -0.3854,  0.0404]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_masked_attention(T, T, T, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.query_size = int(hidden_size / num_heads)\n",
    "        self.lin = cm.Linear(hidden_size, hidden_size)\n",
    "        self.qkv = nn.ModuleList([copy.deepcopy(self.lin) for _ in range(3)])\n",
    "        self.ff = cm.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "        '''\n",
    "        Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "        Q: shape (batch, seq, nheads*headsize)\n",
    "        K: shape (batch, seq, nheads*headsize)\n",
    "        V: shape (batch, seq, nheads*headsize)\n",
    "\n",
    "        returns: shape (batch, seq, nheads*headsize)\n",
    "        '''\n",
    "        Q = rearrange(Q, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "        K = rearrange(K, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "        V = rearrange(V, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "\n",
    "        batch_size, seq_len, nheads, headsize = Q.shape\n",
    "        scores = einsum('B Qseq nheads headsize, B Kseq nheads headsize -> B nheads Qseq Kseq', Q, K)\n",
    "        scores /= Q.shape[-1] ** 0.5\n",
    "\n",
    "        # create lower-left triangle of ones, including the diagonal\n",
    "        mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "        # fill with close-to-neg-inf values where mask==0\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        scores = t.softmax(scores, dim=-1)\n",
    "        Z = einsum('B nheads Qseq Kseq, B Qseq nheads headsize -> B Qseq nheads headsize', scores, V)\n",
    "        Z = rearrange(Z, 'B Qseq nheads headsize -> B Qseq (nheads headsize)')\n",
    "        return Z\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        x = t.cat([x, x, x], dim=-1) # x reshaped to [BATCH_SIZE x SEQ_LEN x 3 * EMB_DIM]\n",
    "\n",
    "        # x projected to [BATCH_SIZE x SEQ_LEN x HEADS * QUERY_SIZE], then reshaped to \n",
    "        # [BATCH_SIZE x SEQ_LEN x HEADS x QUERY_SIZE], and finally permuted to \n",
    "        # [BATCH_SIZE x HEADS x SEQ_LEN x QUERY_SIZE] for all q, k, v\n",
    "        # TODO:\n",
    "\n",
    "        z = self.multihead_masked_attention(q, k, v, num_heads=self.num_heads)\n",
    "        \n",
    "        # z made contiguous in memory and transformed from [BATCH_SIZE x HEADS x SEQ_LEN x QUERY_SIZE]\n",
    "        # to [BATCH_SIZE x SEQ_LEN x HEADS * QUERY_SIZE]\n",
    "        # TODO:\n",
    "\n",
    "        return self.ff(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
