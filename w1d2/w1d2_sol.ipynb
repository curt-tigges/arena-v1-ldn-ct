{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import utils\n",
    "import cnn_modules as cm\n",
    "import transformer_modules as tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of self-attention (see the \"Self-Attention in Detail\" section of the Illustrated Transformer).\n",
    "\n",
    "    With this function, you can ignore masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, embed_size)\n",
    "    K: shape (batch, seq_len, embed_size)\n",
    "    V: shape (batch, seq_len, embed_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, embed_size)\n",
    "    '''\n",
    "    scores = Q @ t.transpose(K, -2, -1)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B Seq Score, B Seq Emb -> B Seq Emb', scores, V)\n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_head_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    Should return the results of masked self-attention.\n",
    "\n",
    "    See \"The Decoder Side\" section of the Illustrated Transformer for an explanation of masking.\n",
    "\n",
    "    Q: shape (batch, seq_len, embed_size)\n",
    "    K: shape (batch, seq_len, embed_size)\n",
    "    V: shape (batch, seq_len, embed_size)\n",
    "\n",
    "    Return: shape (batch, seq_len, embed_size)\n",
    "    '''\n",
    "    batch_size, seq_len, embed_size = Q.shape\n",
    "    scores = Q @ t.transpose(K, -2, -1)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "    \n",
    "    # create lower-left triangle of ones, including the diagonal\n",
    "    mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "    # fill with close-to-neg-inf values\n",
    "    scores = scores.masked_fill(mask==0, -1e9)\n",
    "    \n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B Seq Score, B Seq Emb -> B Seq Emb', scores, V)\n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_masked_attention(Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "    '''\n",
    "    Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "    Q: shape (batch, seq, nheads*headsize)\n",
    "    K: shape (batch, seq, nheads*headsize)\n",
    "    V: shape (batch, seq, nheads*headsize)\n",
    "\n",
    "    returns: shape (batch, seq, nheads*headsize)\n",
    "    '''\n",
    "    Q = rearrange(Q, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "    K = rearrange(K, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "    V = rearrange(V, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "\n",
    "    batch_size, seq_len, nheads, headsize = Q.shape\n",
    "    scores = einsum('B Qseq nheads headsize, B Kseq nheads headsize -> B nheads Qseq Kseq', Q, K)\n",
    "    scores /= Q.shape[-1] ** 0.5\n",
    "\n",
    "    # create lower-left triangle of ones, including the diagonal\n",
    "    mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "    # fill with close-to-neg-inf values where mask==0\n",
    "    scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "    scores = t.softmax(scores, dim=-1)\n",
    "    Z = einsum('B nheads Qseq Kseq, B Qseq nheads headsize -> B Qseq nheads headsize', scores, V)\n",
    "    Z = rearrange(Z, 'B Qseq nheads headsize -> B Qseq (nheads headsize)')\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = t.randn(2, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0690, -0.1922,  0.2547,  0.1276],\n",
       "         [-1.0280,  0.3918,  0.6597,  0.2924],\n",
       "         [ 2.6227, -0.2859,  0.4566, -0.8573],\n",
       "         [ 0.0169,  1.2004,  0.2267,  0.0525],\n",
       "         [-0.4390, -0.0982,  0.2922,  1.3428],\n",
       "         [ 2.5959,  0.8279,  0.3797,  0.9342],\n",
       "         [ 1.6417, -0.4528, -0.4696, -2.2922],\n",
       "         [ 0.2549,  0.7094, -0.3652, -0.3461],\n",
       "         [-0.4314, -1.0372, -0.1216,  0.2004],\n",
       "         [-1.4438,  1.1699, -0.3740,  0.8607]],\n",
       "\n",
       "        [[ 1.4220,  0.3436,  0.0193,  0.6921],\n",
       "         [-1.1452,  0.3468,  0.3634, -0.5552],\n",
       "         [ 0.7866,  0.7927,  1.0079, -0.0143],\n",
       "         [ 1.7841,  0.2455,  0.3745, -0.0996],\n",
       "         [ 0.0591, -0.1512,  0.7890,  0.2173],\n",
       "         [ 0.4247, -0.6400,  0.4725, -0.9128],\n",
       "         [ 0.0356, -0.3364, -0.5728, -0.6655],\n",
       "         [ 0.1734, -1.0741, -1.4653, -0.0248],\n",
       "         [-0.0344, -0.1084,  1.0520, -1.0145],\n",
       "         [-0.4470, -1.5919,  0.0444, -1.1388]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_masked_attention(T, T, T, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadMaskedAttention(nn.Module):\n",
    "    W_QKV: nn.Linear\n",
    "    W_O: nn.Linear\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.query_size = int(hidden_size / num_heads)\n",
    "        self.qkv = cm.Linear(hidden_size, 3*hidden_size)\n",
    "        self.ff = cm.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, num_heads: int):\n",
    "        '''\n",
    "        Implements multihead masked attention on the matrices Q, K and V.\n",
    "\n",
    "        Q: shape (batch, seq, nheads*headsize)\n",
    "        K: shape (batch, seq, nheads*headsize)\n",
    "        V: shape (batch, seq, nheads*headsize)\n",
    "\n",
    "        returns: shape (batch, seq, nheads*headsize)\n",
    "        '''\n",
    "        Q = rearrange(Q, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "        K = rearrange(K, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "        V = rearrange(V, 'B S (nheads headsize) -> B S nheads headsize', nheads = num_heads)\n",
    "\n",
    "        batch_size, seq_len, nheads, headsize = Q.shape\n",
    "        scores = einsum('B Qseq nheads headsize, B Kseq nheads headsize -> B nheads Qseq Kseq', Q, K)\n",
    "        scores /= Q.shape[-1] ** 0.5\n",
    "\n",
    "        # create lower-left triangle of ones, including the diagonal\n",
    "        mask = t.tril(t.ones(seq_len, seq_len), diagonal=0)\n",
    "        # fill with close-to-neg-inf values where mask==0\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        scores = t.softmax(scores, dim=-1)\n",
    "        Z = einsum('B nheads Qseq Kseq, B Qseq nheads headsize -> B Qseq nheads headsize', scores, V)\n",
    "        Z = rearrange(Z, 'B Qseq nheads headsize -> B Qseq (nheads headsize)')\n",
    "        return Z\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        out = self.qkv(x)\n",
    "        Q, K, V = t.tensor_split(out, 3, dim=-1)\n",
    "\n",
    "        Z = self.multihead_masked_attention(Q, K, V, self.num_heads)\n",
    "\n",
    "        return self.ff(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_seq_len: int\n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TransformerConfig(\n",
    "    num_layers=4, \n",
    "    num_heads=2, \n",
    "    vocab_size=500, \n",
    "    hidden_size=64,\n",
    "    max_seq_len=100,\n",
    "    dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = cm.Linear(hidden_size, 4 * hidden_size)\n",
    "        self.gelu = tm.GELU()\n",
    "        self.linear2 = cm.Linear(4 * hidden_size, hidden_size)\n",
    "        self.dropout = tm.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.gelu(self.linear1(x))\n",
    "        out = self.dropout(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.attn = tm.MultiheadMaskedAttention(config.hidden_size, config.num_heads)\n",
    "        self.lnorm1 = tm.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = tm.MLP(config.hidden_size, config.dropout)\n",
    "        self.lnorm2 = tm.LayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        normed_attn = self.lnorm1(self.attn(x))\n",
    "        out = normed_attn + x\n",
    "        normed_mlp = self.lnorm2(self.mlp(out))\n",
    "        out = normed_mlp + out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.emb = tm.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_enc = tm.PositionalEncoding(config.max_seq_len, config.hidden_size)\n",
    "        self.dropout = tm.Dropout(p=config.dropout)\n",
    "\n",
    "        decoders = [DecoderBlock(config) for l in range(config.num_layers)]\n",
    "        self.decoders = nn.Sequential(*decoders)\n",
    "        \n",
    "        self.post_norm = tm.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
    "        embedding = self.emb(x)\n",
    "        embedding = self.pos_enc(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "\n",
    "        out = self.decoders(embedding)\n",
    "        out = self.post_norm(out)\n",
    "\n",
    "        out = einsum(\"B S E, V E -> B S V\", out, self.emb.weight)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('arena')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e28c680d33f95a364b6d7e112cefa96ea26c04ddac857c82a143b1aa5b3dfb2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
